{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"njn8ffyLuu5h","colab_type":"text"},"cell_type":"markdown","source":["# Imports and Setup\n","Run the Code Below"]},{"metadata":{"id":"eY6RgPDNuj5Q","colab_type":"code","outputId":"f1baf3ff-9ce0-4153-ca88-5db286b4d9ef","executionInfo":{"status":"ok","timestamp":1555879408683,"user_tz":240,"elapsed":4428,"user":{"displayName":"Hao Chuan Lei","photoUrl":"","userId":"04091475403023878525"}},"colab":{"base_uri":"https://localhost:8080/","height":145}},"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import learning_curve\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import torch\n","import torchvision\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset\n","import torchvision.transforms as transforms\n","from torch.autograd import Variable\n","import torch\n","import time\n","\n","!git clone https://github.com/cis700/hw1-release.git\n","!mv hw1-release/dills/* .\n","!mv hw1-release hw1\n","\n","from hw1.helper import Logger\n","\n","device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Cloning into 'hw1-release'...\n","remote: Enumerating objects: 26, done.\u001b[K\n","remote: Counting objects:   3% (1/26)   \u001b[K\rremote: Counting objects:   7% (2/26)   \u001b[K\rremote: Counting objects:  11% (3/26)   \u001b[K\rremote: Counting objects:  15% (4/26)   \u001b[K\rremote: Counting objects:  19% (5/26)   \u001b[K\rremote: Counting objects:  23% (6/26)   \u001b[K\rremote: Counting objects:  26% (7/26)   \u001b[K\rremote: Counting objects:  30% (8/26)   \u001b[K\rremote: Counting objects:  34% (9/26)   \u001b[K\rremote: Counting objects:  38% (10/26)   \u001b[K\rremote: Counting objects:  42% (11/26)   \u001b[K\rremote: Counting objects:  46% (12/26)   \u001b[K\rremote: Counting objects:  50% (13/26)   \u001b[K\rremote: Counting objects:  53% (14/26)   \u001b[K\rremote: Counting objects:  57% (15/26)   \u001b[K\rremote: Counting objects:  61% (16/26)   \u001b[K\rremote: Counting objects:  65% (17/26)   \u001b[K\rremote: Counting objects:  69% (18/26)   \u001b[K\rremote: Counting objects:  73% (19/26)   \u001b[K\rremote: Counting objects:  76% (20/26)   \u001b[K\rremote: Counting objects:  80% (21/26)   \u001b[K\rremote: Counting objects:  84% (22/26)   \u001b[K\rremote: Counting objects:  88% (23/26)   \u001b[K\rremote: Counting objects:  92% (24/26)   \u001b[K\rremote: Counting objects:  96% (25/26)   \u001b[K\rremote: Counting objects: 100% (26/26)   \u001b[K\rremote: Counting objects: 100% (26/26), done.\u001b[K\n","remote: Compressing objects:   5% (1/17)   \u001b[K\rremote: Compressing objects:  11% (2/17)   \u001b[K\rremote: Compressing objects:  17% (3/17)   \u001b[K\rremote: Compressing objects:  23% (4/17)   \u001b[K\rremote: Compressing objects:  29% (5/17)   \u001b[K\rremote: Compressing objects:  35% (6/17)   \u001b[K\rremote: Compressing objects:  41% (7/17)   \u001b[K\rremote: Compressing objects:  47% (8/17)   \u001b[K\rremote: Compressing objects:  52% (9/17)   \u001b[K\rremote: Compressing objects:  58% (10/17)   \u001b[K\rremote: Compressing objects:  64% (11/17)   \u001b[K\rremote: Compressing objects:  70% (12/17)   \u001b[K\rremote: Compressing objects:  76% (13/17)   \u001b[K\rremote: Compressing objects:  82% (14/17)   \u001b[K\rremote: Compressing objects:  88% (15/17)   \u001b[K\rremote: Compressing objects:  94% (16/17)   \u001b[K\rremote: Compressing objects: 100% (17/17)   \u001b[K\rremote: Compressing objects: 100% (17/17), done.\u001b[K\n","remote: Total 26 (delta 6), reused 23 (delta 6), pack-reused 0\u001b[K\n","Unpacking objects:   3% (1/26)   \rUnpacking objects:   7% (2/26)   \rUnpacking objects:  11% (3/26)   \rUnpacking objects:  15% (4/26)   \rUnpacking objects:  19% (5/26)   \rUnpacking objects:  23% (6/26)   \rUnpacking objects:  26% (7/26)   \rUnpacking objects:  30% (8/26)   \rUnpacking objects:  34% (9/26)   \rUnpacking objects:  38% (10/26)   \rUnpacking objects:  42% (11/26)   \rUnpacking objects:  46% (12/26)   \rUnpacking objects:  50% (13/26)   \rUnpacking objects:  53% (14/26)   \rUnpacking objects:  57% (15/26)   \rUnpacking objects:  61% (16/26)   \rUnpacking objects:  65% (17/26)   \rUnpacking objects:  69% (18/26)   \rUnpacking objects:  73% (19/26)   \rUnpacking objects:  76% (20/26)   \rUnpacking objects:  80% (21/26)   \rUnpacking objects:  84% (22/26)   \rUnpacking objects:  88% (23/26)   \rUnpacking objects:  92% (24/26)   \rUnpacking objects:  96% (25/26)   \rUnpacking objects: 100% (26/26)   \rUnpacking objects: 100% (26/26), done.\n","cuda:0\n"],"name":"stdout"}]},{"metadata":{"id":"fNpk9oTzve6p","colab_type":"text"},"cell_type":"markdown","source":["## TensorBoard Integration"]},{"metadata":{"id":"Hgos71sOvdp_","colab_type":"code","colab":{}},"cell_type":"code","source":["#! rm -r ./logs\n","LOG_DIR = './logs'\n","get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","    .format(LOG_DIR)\n",")\n","\n","!if [ -f ngrok ] ; then echo \"Ngrok already installed\" ; else wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip > /dev/null 2>&1 && unzip ngrok-stable-linux-amd64.zip > /dev/null 2>&1 ; fi\n","\n","get_ipython().system_raw('./ngrok http 6006 &')\n","\n","! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print('Tensorboard Link: ' +str(json.load(sys.stdin)['tunnels'][0]['public_url']))\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"u_RajyOgvn_J","colab_type":"text"},"cell_type":"markdown","source":["## Google Drive Integration\n","For Google Drive use only"]},{"metadata":{"id":"QxKL7yGxvuKx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":146},"outputId":"bfb3ed2b-176f-450a-c8f8-314b270e0842","executionInfo":{"status":"ok","timestamp":1555878516745,"user_tz":240,"elapsed":21113,"user":{"displayName":"Hao Chuan Lei","photoUrl":"","userId":"04091475403023878525"}}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","local_dir= 'drive/My Drive/CIS700-004/project'\n","%cd 'drive/My Drive/CIS700-004/project'"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","/content/drive/My Drive/CIS700-004/project\n"],"name":"stdout"}]},{"metadata":{"id":"uC8mUVvNwXSR","colab_type":"text"},"cell_type":"markdown","source":["# The Code"]},{"metadata":{"id":"xMwhR-s9bZWL","colab_type":"text"},"cell_type":"markdown","source":["## LSTM"]},{"metadata":{"id":"89jT-97yZvyz","colab_type":"code","colab":{}},"cell_type":"code","source":["class LSTM(nn.Module): \n","  def __init__(self, input_size, hidden_size, num_layers, bidirectional= False):\n","    super(LSTM, self).__init__()\n","    self.hidden_size= hidden_size\n","    self.num_layers= num_layers\n","    self.lstm= nn.LSTM(input_size, hidden_size, \n","                       num_layers, batch_first= True, bidirectional= bidirectional)\n","    \n","    self.fc= nn.Linear(hidden_size, 2)\n","  def forward(self, x): \n","    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","    c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","    \n","    out, _ = self.lstm(x, (h0, c0))\n","    out= self.fc(out[:, -1, :]) #hidden state of last time step\n","    return out\n","\n","lstm = LSTM(300, 400, 3, False).to(device)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oiEIL5VxmDym","colab_type":"text"},"cell_type":"markdown","source":["## RunNet"]},{"metadata":{"id":"Rlw8RP2DbeKm","colab_type":"code","colab":{}},"cell_type":"code","source":["class RunNet():\n","  def __init__(self, net, train_load, test_load, lr= 0.0001, conv= False):\n","    self.logger= Logger('./logs')\n","    self.conv= conv\n","    self.net= net\n","    self.train_load= train_load\n","    self.test_load= test_load\n","    self.criterion = nn.CrossEntropyLoss()\n","    self.optimizer= torch.optim.Adam(self.net.parameters(), lr= lr)\n","  \n","  def test_accuracy(self):\n","    test_c= 0\n","    test_t= 0\n","    with torch.no_grad(): \n","      for i, data in enumerate(self.test_load, 0):\n","        inputs, labels= data\n","        inputs, labels= Variable(inputs), Variable(labels)\n","\n","        inputs= inputs.to(device)\n","        labels= labels.to(device)\n","        \n","        if not self.conv:\n","          inputs = inputs.view(inputs.shape[0], -1)\n","\n","        pred= self.net(inputs)\n","        _, pred= torch.max(pred, 1)\n","        test_c= test_c + int((pred==labels).sum())\n","        test_t= test_t + labels.size(0)\n","      print(\"Test Acc:\", (test_c/test_t))\n","      return test_c/test_t\n","\n","    \n","  def run(self, max_epochs= 200, log_every= 10, verbose= True):\n","    step= 0\n","    for epoch in range(max_epochs): \n","        if verbose: \n","          print(\"Epoch:\", epoch)\n","\n","        train_c= 0\n","        train_t= 0\n","\n","        test_c= 0\n","        test_t= 0\n","\n","        for i, data in enumerate(self.train_load, 0):\n","            inputs, labels= data\n","            inputs, labels= Variable(inputs), Variable(labels)\n","\n","            inputs= inputs.to(device)\n","            labels= labels.to(device)\n","            \n","            if not self.conv:  \n","              inputs = inputs.view(inputs.shape[0], -1)\n","\n","            #feed inputs into network\n","            y_pred = self.net(inputs)\n","            loss= self.criterion(y_pred, labels)\n","\n","            #zero gradient\n","            self.optimizer.zero_grad()\n","\n","            #backprop\n","            loss.backward()\n","\n","            #weight update\n","            self.optimizer.step()\n","\n","            #after updating... report training accuracy\n","            pred= self.net(inputs)\n","            t_loss= self.criterion(pred, labels)\n","            _, pred= torch.max(pred, 1)\n","\n","            train_correct= int((pred==labels).sum())\n","            train_c= train_c + train_correct\n","            train_t = train_t + labels.size(0)\n","\n","            if (step % log_every == 0):\n","              self.logger.scalar_summary(\"training accuracy\", (train_correct/labels.size(0)), step)\n","              self.logger.scalar_summary(\"loss\", t_loss.item(), step)\n","              self.logger.writer.flush()\n","              print(\"train: %s, loss: %s\"%(train_c/train_t, t_loss.item()))\n","            step= step + 1\n","        with torch.no_grad():      \n","            for i, data in enumerate(self.test_load, 0):\n","                inputs, labels= data\n","                inputs, labels= Variable(inputs), Variable(labels)\n","\n","                inputs= inputs.to(device)\n","                labels= labels.to(device)\n","                \n","                if not self.conv:\n","                  inputs = inputs.view(inputs.shape[0], -1)\n","\n","                pred= self.net(inputs)\n","                _, pred= torch.max(pred, 1)\n","                test_correct= int((pred==labels).sum())\n","                test_c= test_c + test_correct\n","                test_t= test_t + labels.size(0)\n","\n","        if verbose: \n","          print(\"Train Acc:\", (train_c/train_t),\"Test Acc:\", (test_c/test_t), \n","                \"Loss:\", t_loss.item())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YwP3I60BmgsB","colab_type":"text"},"cell_type":"markdown","source":["# Test Run"]},{"metadata":{"id":"azhf1iC7mgWf","colab_type":"code","colab":{}},"cell_type":"code","source":["class CustomDataset(Dataset):\n","  '''\n","  DO NOT EDIT THIS CLASS\n","  '''\n","  def __init__(self, X, y):\n","      self.len = len(X)           \n","      self.x_data = torch.from_numpy(X).float()\n","      self.y_data = torch.from_numpy(y).long()\n","\n","  def __len__(self):\n","      return self.len\n","\n","  def __getitem__(self, idx):\n","      return self.x_data[idx], self.y_data[idx]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"v3zXdvjGm7y9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":563},"outputId":"862da1a3-5112-4f7e-cc66-58e5c5608005","executionInfo":{"status":"ok","timestamp":1555879960225,"user_tz":240,"elapsed":310,"user":{"displayName":"Hao Chuan Lei","photoUrl":"","userId":"04091475403023878525"}}},"cell_type":"code","source":["x= np.array([np.random.rand(1, 300) for i in range(1000)])\n","y= np.array([(int)(np.mean(i)/0.5) for i in x])\n","print(y)\n","train_data= CustomDataset(x, y)\n","x2= np.array([np.random.rand(1, 300) for i in range(1000)])\n","y2= np.array([(int)(np.mean(i)/0.5) for i in x2])\n","test_data= CustomDataset(x2, y2)\n","\n","train_load= DataLoader(train_data, batch_size= 100)\n","test_load= DataLoader(test_data, batch_size= 100)\n","\n","print(len(train_load.dataset))\n","print(len(test_load.dataset))"],"execution_count":42,"outputs":[{"output_type":"stream","text":["[0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1\n"," 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1\n"," 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0\n"," 1 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0\n"," 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 1\n"," 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0\n"," 1 0 1 1 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0\n"," 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0\n"," 0 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1\n"," 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1\n"," 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1\n"," 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1\n"," 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 1 0 1 0 0\n"," 0 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0 1 0 1 0\n"," 1 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0\n"," 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1\n"," 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1\n"," 0 1 1 0 1 1 1 0 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1\n"," 1 1 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1\n"," 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 0 0 0 0 0 1 1 1 1\n"," 1 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0\n"," 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1\n"," 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 0 0 0\n"," 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 1 0\n"," 1 1 0 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0\n"," 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0\n"," 1 0 0 1 1 0 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0 0 0\n"," 0]\n","1000\n","1000\n"],"name":"stdout"}]},{"metadata":{"id":"PyqG-zsWpM4R","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":10926},"outputId":"5c014581-e194-49b9-c622-c9af1533c282","executionInfo":{"status":"ok","timestamp":1555880004825,"user_tz":240,"elapsed":14419,"user":{"displayName":"Hao Chuan Lei","photoUrl":"","userId":"04091475403023878525"}}},"cell_type":"code","source":["run_net= RunNet(lstm, train_load, test_load, conv= True)\n","run_net.run()"],"execution_count":45,"outputs":[{"output_type":"stream","text":["Epoch: 0\n","train: 0.67, loss: 1.1307047605514526\n","Train Acc: 0.886 Test Acc: 0.807 Loss: 0.08376891165971756\n","Epoch: 1\n","train: 1.0, loss: 0.04930339753627777\n","Train Acc: 0.965 Test Acc: 0.79 Loss: 0.054295677691698074\n","Epoch: 2\n","train: 1.0, loss: 0.06195160001516342\n","Train Acc: 0.974 Test Acc: 0.785 Loss: 0.06024729087948799\n","Epoch: 3\n","train: 0.99, loss: 0.07583547383546829\n","Train Acc: 0.975 Test Acc: 0.772 Loss: 0.07751547545194626\n","Epoch: 4\n","train: 0.99, loss: 0.09026353806257248\n","Train Acc: 0.975 Test Acc: 0.759 Loss: 0.10137949883937836\n","Epoch: 5\n","train: 0.98, loss: 0.09730991721153259\n","Train Acc: 0.973 Test Acc: 0.752 Loss: 0.12060822546482086\n","Epoch: 6\n","train: 0.99, loss: 0.0935828760266304\n","Train Acc: 0.978 Test Acc: 0.748 Loss: 0.12727224826812744\n","Epoch: 7\n","train: 0.99, loss: 0.0858883187174797\n","Train Acc: 0.976 Test Acc: 0.746 Loss: 0.1253443956375122\n","Epoch: 8\n","train: 0.99, loss: 0.0813286080956459\n","Train Acc: 0.978 Test Acc: 0.747 Loss: 0.1220644935965538\n","Epoch: 9\n","train: 1.0, loss: 0.08145737648010254\n","Train Acc: 0.981 Test Acc: 0.747 Loss: 0.12154171615839005\n","Epoch: 10\n","train: 1.0, loss: 0.08467128127813339\n","Train Acc: 0.98 Test Acc: 0.745 Loss: 0.12440958619117737\n","Epoch: 11\n","train: 1.0, loss: 0.08841631561517715\n","Train Acc: 0.98 Test Acc: 0.745 Loss: 0.12861734628677368\n","Epoch: 12\n","train: 1.0, loss: 0.091065913438797\n","Train Acc: 0.978 Test Acc: 0.742 Loss: 0.13220298290252686\n","Epoch: 13\n","train: 1.0, loss: 0.09322024136781693\n","Train Acc: 0.978 Test Acc: 0.739 Loss: 0.1355295330286026\n","Epoch: 14\n","train: 1.0, loss: 0.09620688110589981\n","Train Acc: 0.978 Test Acc: 0.736 Loss: 0.13970844447612762\n","Epoch: 15\n","train: 1.0, loss: 0.10017016530036926\n","Train Acc: 0.975 Test Acc: 0.732 Loss: 0.14469072222709656\n","Epoch: 16\n","train: 0.98, loss: 0.10462293028831482\n","Train Acc: 0.972 Test Acc: 0.729 Loss: 0.1499563455581665\n","Epoch: 17\n","train: 0.97, loss: 0.10965388268232346\n","Train Acc: 0.969 Test Acc: 0.727 Loss: 0.15557406842708588\n","Epoch: 18\n","train: 0.96, loss: 0.11564318835735321\n","Train Acc: 0.966 Test Acc: 0.725 Loss: 0.16169650852680206\n","Epoch: 19\n","train: 0.95, loss: 0.12269537150859833\n","Train Acc: 0.964 Test Acc: 0.718 Loss: 0.16816867887973785\n","Epoch: 20\n","train: 0.95, loss: 0.13093337416648865\n","Train Acc: 0.963 Test Acc: 0.715 Loss: 0.174895241856575\n","Epoch: 21\n","train: 0.95, loss: 0.14065410196781158\n","Train Acc: 0.961 Test Acc: 0.715 Loss: 0.18183332681655884\n","Epoch: 22\n","train: 0.95, loss: 0.1521541029214859\n","Train Acc: 0.96 Test Acc: 0.714 Loss: 0.18882474303245544\n","Epoch: 23\n","train: 0.91, loss: 0.16578921675682068\n","Train Acc: 0.954 Test Acc: 0.711 Loss: 0.19567671418190002\n","Epoch: 24\n","train: 0.91, loss: 0.1820477694272995\n","Train Acc: 0.949 Test Acc: 0.71 Loss: 0.20212921500205994\n","Epoch: 25\n","train: 0.88, loss: 0.20153574645519257\n","Train Acc: 0.942 Test Acc: 0.709 Loss: 0.20777802169322968\n","Epoch: 26\n","train: 0.86, loss: 0.22504569590091705\n","Train Acc: 0.937 Test Acc: 0.708 Loss: 0.2120341658592224\n","Epoch: 27\n","train: 0.83, loss: 0.2536272704601288\n","Train Acc: 0.926 Test Acc: 0.709 Loss: 0.2140207290649414\n","Epoch: 28\n","train: 0.82, loss: 0.2886751592159271\n","Train Acc: 0.919 Test Acc: 0.71 Loss: 0.21247053146362305\n","Epoch: 29\n","train: 0.78, loss: 0.33209192752838135\n","Train Acc: 0.902 Test Acc: 0.716 Loss: 0.20572921633720398\n","Epoch: 30\n","train: 0.76, loss: 0.38666629791259766\n","Train Acc: 0.886 Test Acc: 0.725 Loss: 0.19201669096946716\n","Epoch: 31\n","train: 0.75, loss: 0.45727652311325073\n","Train Acc: 0.859 Test Acc: 0.738 Loss: 0.16857047379016876\n","Epoch: 32\n","train: 0.69, loss: 0.5521101355552673\n","Train Acc: 0.824 Test Acc: 0.759 Loss: 0.11721590161323547\n","Epoch: 33\n","train: 0.67, loss: 0.6292171478271484\n","Train Acc: 0.772 Test Acc: 0.804 Loss: 0.08438896387815475\n","Epoch: 34\n","train: 0.83, loss: 0.24637813866138458\n","Train Acc: 0.755 Test Acc: 0.711 Loss: 0.7732557654380798\n","Epoch: 35\n","train: 0.86, loss: 0.21847127377986908\n","Train Acc: 0.838 Test Acc: 0.725 Loss: 0.6274820566177368\n","Epoch: 36\n","train: 0.78, loss: 0.5844075679779053\n","Train Acc: 0.919 Test Acc: 0.805 Loss: 0.08020863682031631\n","Epoch: 37\n","train: 0.95, loss: 0.12947101891040802\n","Train Acc: 0.985 Test Acc: 0.808 Loss: 0.09671510756015778\n","Epoch: 38\n","train: 0.97, loss: 0.10324766486883163\n","Train Acc: 0.971 Test Acc: 0.812 Loss: 0.15979263186454773\n","Epoch: 39\n","train: 0.92, loss: 0.1752186417579651\n","Train Acc: 0.97 Test Acc: 0.805 Loss: 0.09308365732431412\n","Epoch: 40\n","train: 0.95, loss: 0.12206745147705078\n","Train Acc: 0.985 Test Acc: 0.806 Loss: 0.0923587828874588\n","Epoch: 41\n","train: 0.95, loss: 0.10631100833415985\n","Train Acc: 0.983 Test Acc: 0.811 Loss: 0.10195017606019974\n","Epoch: 42\n","train: 0.95, loss: 0.11534350365400314\n","Train Acc: 0.983 Test Acc: 0.811 Loss: 0.08874669671058655\n","Epoch: 43\n","train: 0.95, loss: 0.10370869934558868\n","Train Acc: 0.989 Test Acc: 0.811 Loss: 0.08266694843769073\n","Epoch: 44\n","train: 0.97, loss: 0.09311190247535706\n","Train Acc: 0.992 Test Acc: 0.811 Loss: 0.0816141813993454\n","Epoch: 45\n","train: 0.97, loss: 0.08926649391651154\n","Train Acc: 0.993 Test Acc: 0.813 Loss: 0.07720696181058884\n","Epoch: 46\n","train: 0.98, loss: 0.08363860845565796\n","Train Acc: 0.994 Test Acc: 0.813 Loss: 0.072356678545475\n","Epoch: 47\n","train: 0.98, loss: 0.0768762081861496\n","Train Acc: 0.995 Test Acc: 0.813 Loss: 0.06895866245031357\n","Epoch: 48\n","train: 0.98, loss: 0.0714472234249115\n","Train Acc: 0.995 Test Acc: 0.816 Loss: 0.06575153023004532\n","Epoch: 49\n","train: 0.98, loss: 0.06669475883245468\n","Train Acc: 0.995 Test Acc: 0.818 Loss: 0.06235340237617493\n","Epoch: 50\n","train: 0.98, loss: 0.06199735775589943\n","Train Acc: 0.996 Test Acc: 0.816 Loss: 0.05918865278363228\n","Epoch: 51\n","train: 0.98, loss: 0.05761711671948433\n","Train Acc: 0.996 Test Acc: 0.817 Loss: 0.056332558393478394\n","Epoch: 52\n","train: 0.98, loss: 0.05371052771806717\n","Train Acc: 0.996 Test Acc: 0.818 Loss: 0.05364524945616722\n","Epoch: 53\n","train: 0.98, loss: 0.050176333636045456\n","Train Acc: 0.997 Test Acc: 0.819 Loss: 0.05109502002596855\n","Epoch: 54\n","train: 0.99, loss: 0.046945519745349884\n","Train Acc: 0.998 Test Acc: 0.821 Loss: 0.048712871968746185\n","Epoch: 55\n","train: 1.0, loss: 0.04401902109384537\n","Train Acc: 0.999 Test Acc: 0.825 Loss: 0.04649900272488594\n","Epoch: 56\n","train: 1.0, loss: 0.04138723388314247\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.044433485716581345\n","Epoch: 57\n","train: 1.0, loss: 0.03901815786957741\n","Train Acc: 1.0 Test Acc: 0.829 Loss: 0.04250388965010643\n","Epoch: 58\n","train: 1.0, loss: 0.03688083589076996\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.040704380720853806\n","Epoch: 59\n","train: 1.0, loss: 0.034951742738485336\n","Train Acc: 1.0 Test Acc: 0.831 Loss: 0.03902755677700043\n","Epoch: 60\n","train: 1.0, loss: 0.03320954367518425\n","Train Acc: 1.0 Test Acc: 0.831 Loss: 0.037463780492544174\n","Epoch: 61\n","train: 1.0, loss: 0.03163262829184532\n","Train Acc: 1.0 Test Acc: 0.831 Loss: 0.03600379079580307\n","Epoch: 62\n","train: 1.0, loss: 0.030201105400919914\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.03463933616876602\n","Epoch: 63\n","train: 1.0, loss: 0.028897572308778763\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.03336270898580551\n","Epoch: 64\n","train: 1.0, loss: 0.02770686149597168\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.03216640651226044\n","Epoch: 65\n","train: 1.0, loss: 0.026615571230649948\n","Train Acc: 1.0 Test Acc: 0.831 Loss: 0.03104357421398163\n","Epoch: 66\n","train: 1.0, loss: 0.02561185322701931\n","Train Acc: 1.0 Test Acc: 0.831 Loss: 0.029987778514623642\n","Epoch: 67\n","train: 1.0, loss: 0.024685464799404144\n","Train Acc: 1.0 Test Acc: 0.831 Loss: 0.02899346314370632\n","Epoch: 68\n","train: 1.0, loss: 0.023827500641345978\n","Train Acc: 1.0 Test Acc: 0.831 Loss: 0.028055299073457718\n","Epoch: 69\n","train: 1.0, loss: 0.023030271753668785\n","Train Acc: 1.0 Test Acc: 0.832 Loss: 0.027168625965714455\n","Epoch: 70\n","train: 1.0, loss: 0.022287044674158096\n","Train Acc: 1.0 Test Acc: 0.833 Loss: 0.02632920816540718\n","Epoch: 71\n","train: 1.0, loss: 0.02159201167523861\n","Train Acc: 1.0 Test Acc: 0.832 Loss: 0.025533227249979973\n","Epoch: 72\n","train: 1.0, loss: 0.020940160378813744\n","Train Acc: 1.0 Test Acc: 0.831 Loss: 0.024777302518486977\n","Epoch: 73\n","train: 1.0, loss: 0.02032705768942833\n","Train Acc: 1.0 Test Acc: 0.831 Loss: 0.024058260023593903\n","Epoch: 74\n","train: 1.0, loss: 0.019748911261558533\n","Train Acc: 1.0 Test Acc: 0.832 Loss: 0.023373447358608246\n","Epoch: 75\n","train: 1.0, loss: 0.019202349707484245\n","Train Acc: 1.0 Test Acc: 0.832 Loss: 0.022720228880643845\n","Epoch: 76\n","train: 1.0, loss: 0.018684448674321175\n","Train Acc: 1.0 Test Acc: 0.832 Loss: 0.022096410393714905\n","Epoch: 77\n","train: 1.0, loss: 0.01819266751408577\n","Train Acc: 1.0 Test Acc: 0.832 Loss: 0.021499965339899063\n","Epoch: 78\n","train: 1.0, loss: 0.017724666744470596\n","Train Acc: 1.0 Test Acc: 0.832 Loss: 0.020928990095853806\n","Epoch: 79\n","train: 1.0, loss: 0.017278460785746574\n","Train Acc: 1.0 Test Acc: 0.832 Loss: 0.020381920039653778\n","Epoch: 80\n","train: 1.0, loss: 0.01685236766934395\n","Train Acc: 1.0 Test Acc: 0.833 Loss: 0.019857097417116165\n","Epoch: 81\n","train: 1.0, loss: 0.01644471101462841\n","Train Acc: 1.0 Test Acc: 0.833 Loss: 0.019353168085217476\n","Epoch: 82\n","train: 1.0, loss: 0.01605415530502796\n","Train Acc: 1.0 Test Acc: 0.833 Loss: 0.018868887796998024\n","Epoch: 83\n","train: 1.0, loss: 0.015679456293582916\n","Train Acc: 1.0 Test Acc: 0.832 Loss: 0.018403073772788048\n","Epoch: 84\n","train: 1.0, loss: 0.01531946286559105\n","Train Acc: 1.0 Test Acc: 0.832 Loss: 0.017954684793949127\n","Epoch: 85\n","train: 1.0, loss: 0.014973154291510582\n","Train Acc: 1.0 Test Acc: 0.832 Loss: 0.017522698268294334\n","Epoch: 86\n","train: 1.0, loss: 0.014639686793088913\n","Train Acc: 1.0 Test Acc: 0.832 Loss: 0.017106221988797188\n","Epoch: 87\n","train: 1.0, loss: 0.014318209141492844\n","Train Acc: 1.0 Test Acc: 0.832 Loss: 0.016704484820365906\n","Epoch: 88\n","train: 1.0, loss: 0.014007959514856339\n","Train Acc: 1.0 Test Acc: 0.832 Loss: 0.016316568478941917\n","Epoch: 89\n","train: 1.0, loss: 0.013708279468119144\n","Train Acc: 1.0 Test Acc: 0.831 Loss: 0.01594194956123829\n","Epoch: 90\n","train: 1.0, loss: 0.013418614864349365\n","Train Acc: 1.0 Test Acc: 0.831 Loss: 0.015579845756292343\n","Epoch: 91\n","train: 1.0, loss: 0.013138282112777233\n","Train Acc: 1.0 Test Acc: 0.832 Loss: 0.01522971410304308\n","Epoch: 92\n","train: 1.0, loss: 0.012866887263953686\n","Train Acc: 1.0 Test Acc: 0.832 Loss: 0.01489094365388155\n","Epoch: 93\n","train: 1.0, loss: 0.012603865005075932\n","Train Acc: 1.0 Test Acc: 0.831 Loss: 0.014562995173037052\n","Epoch: 94\n","train: 1.0, loss: 0.01234884187579155\n","Train Acc: 1.0 Test Acc: 0.831 Loss: 0.014245428144931793\n","Epoch: 95\n","train: 1.0, loss: 0.012101422995328903\n","Train Acc: 1.0 Test Acc: 0.831 Loss: 0.013937736861407757\n","Epoch: 96\n","train: 1.0, loss: 0.011861167848110199\n","Train Acc: 1.0 Test Acc: 0.832 Loss: 0.013639523647725582\n","Epoch: 97\n","train: 1.0, loss: 0.011627762578427792\n","Train Acc: 1.0 Test Acc: 0.832 Loss: 0.013350349850952625\n","Epoch: 98\n","train: 1.0, loss: 0.011400890536606312\n","Train Acc: 1.0 Test Acc: 0.832 Loss: 0.013069862499833107\n","Epoch: 99\n","train: 1.0, loss: 0.011180302128195763\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.01279767882078886\n","Epoch: 100\n","train: 1.0, loss: 0.010965672321617603\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.012533452361822128\n","Epoch: 101\n","train: 1.0, loss: 0.010756712406873703\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.012276891618967056\n","Epoch: 102\n","train: 1.0, loss: 0.010553249157965183\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.012027709744870663\n","Epoch: 103\n","train: 1.0, loss: 0.010355050675570965\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.011785578913986683\n","Epoch: 104\n","train: 1.0, loss: 0.01016186736524105\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.011550234630703926\n","Epoch: 105\n","train: 1.0, loss: 0.009973544627428055\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.011321443133056164\n","Epoch: 106\n","train: 1.0, loss: 0.009789868257939816\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.011098948307335377\n","Epoch: 107\n","train: 1.0, loss: 0.009610726498067379\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.010882494039833546\n","Epoch: 108\n","train: 1.0, loss: 0.009435865096747875\n","Train Acc: 1.0 Test Acc: 0.829 Loss: 0.0106718884781003\n","Epoch: 109\n","train: 1.0, loss: 0.009265213273465633\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.01046693418174982\n","Epoch: 110\n","train: 1.0, loss: 0.00909859873354435\n","Train Acc: 1.0 Test Acc: 0.831 Loss: 0.010267376899719238\n","Epoch: 111\n","train: 1.0, loss: 0.008935869671404362\n","Train Acc: 1.0 Test Acc: 0.831 Loss: 0.010073053650557995\n","Epoch: 112\n","train: 1.0, loss: 0.008776942268013954\n","Train Acc: 1.0 Test Acc: 0.831 Loss: 0.009883766062557697\n","Epoch: 113\n","train: 1.0, loss: 0.008621667511761189\n","Train Acc: 1.0 Test Acc: 0.832 Loss: 0.009699356742203236\n","Epoch: 114\n","train: 1.0, loss: 0.008469960652291775\n","Train Acc: 1.0 Test Acc: 0.832 Loss: 0.009519639424979687\n","Epoch: 115\n","train: 1.0, loss: 0.008321681059896946\n","Train Acc: 1.0 Test Acc: 0.832 Loss: 0.009344480000436306\n","Epoch: 116\n","train: 1.0, loss: 0.008176753297448158\n","Train Acc: 1.0 Test Acc: 0.832 Loss: 0.009173670783638954\n","Epoch: 117\n","train: 1.0, loss: 0.008035079576075077\n","Train Acc: 1.0 Test Acc: 0.831 Loss: 0.009007113054394722\n","Epoch: 118\n","train: 1.0, loss: 0.007896563038229942\n","Train Acc: 1.0 Test Acc: 0.831 Loss: 0.008844644762575626\n","Epoch: 119\n","train: 1.0, loss: 0.007761095650494099\n","Train Acc: 1.0 Test Acc: 0.831 Loss: 0.008686115965247154\n","Epoch: 120\n","train: 1.0, loss: 0.007628621067851782\n","Train Acc: 1.0 Test Acc: 0.831 Loss: 0.008531448431313038\n","Epoch: 121\n","train: 1.0, loss: 0.007499047555029392\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.00838047731667757\n","Epoch: 122\n","train: 1.0, loss: 0.007372294552624226\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.008233103901147842\n","Epoch: 123\n","train: 1.0, loss: 0.007248283829540014\n","Train Acc: 1.0 Test Acc: 0.831 Loss: 0.008089176379144192\n","Epoch: 124\n","train: 1.0, loss: 0.007126950193196535\n","Train Acc: 1.0 Test Acc: 0.831 Loss: 0.007948615588247776\n","Epoch: 125\n","train: 1.0, loss: 0.007008235435932875\n","Train Acc: 1.0 Test Acc: 0.831 Loss: 0.007811354473233223\n","Epoch: 126\n","train: 1.0, loss: 0.006892034783959389\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.007677233312278986\n","Epoch: 127\n","train: 1.0, loss: 0.006778303533792496\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.007546163629740477\n","Epoch: 128\n","train: 1.0, loss: 0.006666990462690592\n","Train Acc: 1.0 Test Acc: 0.829 Loss: 0.007418093737214804\n","Epoch: 129\n","train: 1.0, loss: 0.006558026187121868\n","Train Acc: 1.0 Test Acc: 0.829 Loss: 0.007292881142348051\n","Epoch: 130\n","train: 1.0, loss: 0.0064513590186834335\n","Train Acc: 1.0 Test Acc: 0.829 Loss: 0.0071704494766891\n","Epoch: 131\n","train: 1.0, loss: 0.006346890702843666\n","Train Acc: 1.0 Test Acc: 0.829 Loss: 0.007050783839076757\n","Epoch: 132\n","train: 1.0, loss: 0.006244596093893051\n","Train Acc: 1.0 Test Acc: 0.829 Loss: 0.0069337463937699795\n","Epoch: 133\n","train: 1.0, loss: 0.006144431885331869\n","Train Acc: 1.0 Test Acc: 0.829 Loss: 0.006819257512688637\n","Epoch: 134\n","train: 1.0, loss: 0.00604630820453167\n","Train Acc: 1.0 Test Acc: 0.829 Loss: 0.006707258056849241\n","Epoch: 135\n","train: 1.0, loss: 0.005950235296040773\n","Train Acc: 1.0 Test Acc: 0.829 Loss: 0.006597689352929592\n","Epoch: 136\n","train: 1.0, loss: 0.005856081377714872\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.0064904713071882725\n","Epoch: 137\n","train: 1.0, loss: 0.005763866938650608\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.006385550368577242\n","Epoch: 138\n","train: 1.0, loss: 0.005673488602042198\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.006282844580709934\n","Epoch: 139\n","train: 1.0, loss: 0.005584969650954008\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.006182300858199596\n","Epoch: 140\n","train: 1.0, loss: 0.005498199257999659\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.006083879619836807\n","Epoch: 141\n","train: 1.0, loss: 0.005413160193711519\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.005987501237541437\n","Epoch: 142\n","train: 1.0, loss: 0.005329817533493042\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.005893101915717125\n","Epoch: 143\n","train: 1.0, loss: 0.0052481116726994514\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.005800673738121986\n","Epoch: 144\n","train: 1.0, loss: 0.005167997907847166\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.005710127297788858\n","Epoch: 145\n","train: 1.0, loss: 0.0050894939340651035\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.005621424876153469\n","Epoch: 146\n","train: 1.0, loss: 0.005012502893805504\n","Train Acc: 1.0 Test Acc: 0.829 Loss: 0.0055345045402646065\n","Epoch: 147\n","train: 1.0, loss: 0.004937019199132919\n","Train Acc: 1.0 Test Acc: 0.829 Loss: 0.005449356976896524\n","Epoch: 148\n","train: 1.0, loss: 0.004862959496676922\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.005365885328501463\n","Epoch: 149\n","train: 1.0, loss: 0.004790353588759899\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.005284093786031008\n","Epoch: 150\n","train: 1.0, loss: 0.004719157237559557\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.005203914828598499\n","Epoch: 151\n","train: 1.0, loss: 0.004649280104786158\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.005125309340655804\n","Epoch: 152\n","train: 1.0, loss: 0.0045807622373104095\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.005048240534961224\n","Epoch: 153\n","train: 1.0, loss: 0.004513527266681194\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.004972690250724554\n","Epoch: 154\n","train: 1.0, loss: 0.004447541199624538\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.0048986030742526054\n","Epoch: 155\n","train: 1.0, loss: 0.004382817540317774\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.004825937561690807\n","Epoch: 156\n","train: 1.0, loss: 0.004319285042583942\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.00475467462092638\n","Epoch: 157\n","train: 1.0, loss: 0.004256939981132746\n","Train Acc: 1.0 Test Acc: 0.83 Loss: 0.0046847425401210785\n","Epoch: 158\n","train: 1.0, loss: 0.004195755813270807\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.004616154357790947\n","Epoch: 159\n","train: 1.0, loss: 0.004135661292821169\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.004548866767436266\n","Epoch: 160\n","train: 1.0, loss: 0.004076707176864147\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.004482840187847614\n","Epoch: 161\n","train: 1.0, loss: 0.004018836189061403\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.00441804388538003\n","Epoch: 162\n","train: 1.0, loss: 0.003961982671171427\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.004354467615485191\n","Epoch: 163\n","train: 1.0, loss: 0.0039061892312020063\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.004292074590921402\n","Epoch: 164\n","train: 1.0, loss: 0.0038513648323714733\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.0042308056727051735\n","Epoch: 165\n","train: 1.0, loss: 0.00379755487665534\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.00417069299146533\n","Epoch: 166\n","train: 1.0, loss: 0.0037446701899170876\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.004111659713089466\n","Epoch: 167\n","train: 1.0, loss: 0.0036927652545273304\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.004053695127367973\n","Epoch: 168\n","train: 1.0, loss: 0.003641763934865594\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.003996774088591337\n","Epoch: 169\n","train: 1.0, loss: 0.0035916471388190985\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.003940886352211237\n","Epoch: 170\n","train: 1.0, loss: 0.0035424293018877506\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.0038859962951391935\n","Epoch: 171\n","train: 1.0, loss: 0.0034940659534186125\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.003832075512036681\n","Epoch: 172\n","train: 1.0, loss: 0.003446533577516675\n","Train Acc: 1.0 Test Acc: 0.827 Loss: 0.0037791384384036064\n","Epoch: 173\n","train: 1.0, loss: 0.0033998561557382345\n","Train Acc: 1.0 Test Acc: 0.827 Loss: 0.0037270879838615656\n","Epoch: 174\n","train: 1.0, loss: 0.003353952197358012\n","Train Acc: 1.0 Test Acc: 0.827 Loss: 0.0036759984213858843\n","Epoch: 175\n","train: 1.0, loss: 0.003308839863166213\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.003625782672315836\n","Epoch: 176\n","train: 1.0, loss: 0.003264502389356494\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.0035764514468610287\n","Epoch: 177\n","train: 1.0, loss: 0.0032209157943725586\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.0035279691219329834\n","Epoch: 178\n","train: 1.0, loss: 0.0031780933495610952\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.0034803198650479317\n","Epoch: 179\n","train: 1.0, loss: 0.0031359540298581123\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.0034335076343268156\n","Epoch: 180\n","train: 1.0, loss: 0.0030945646576583385\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.003387482138350606\n","Epoch: 181\n","train: 1.0, loss: 0.0030538057908415794\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.003342224285006523\n","Epoch: 182\n","train: 1.0, loss: 0.0030138015281409025\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.0032977652736008167\n","Epoch: 183\n","train: 1.0, loss: 0.002974405186250806\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.0032540392130613327\n","Epoch: 184\n","train: 1.0, loss: 0.002935700351372361\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.0032110584434121847\n","Epoch: 185\n","train: 1.0, loss: 0.002897593891248107\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.003168793860822916\n","Epoch: 186\n","train: 1.0, loss: 0.0028601575177162886\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.0031272410415112972\n","Epoch: 187\n","train: 1.0, loss: 0.002823283663019538\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.0030863548163324594\n","Epoch: 188\n","train: 1.0, loss: 0.0027870344929397106\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.003046144265681505\n","Epoch: 189\n","train: 1.0, loss: 0.002751374151557684\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.003006635932251811\n","Epoch: 190\n","train: 1.0, loss: 0.0027162719052284956\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.0029677534475922585\n","Epoch: 191\n","train: 1.0, loss: 0.002681741723790765\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.0029294872656464577\n","Epoch: 192\n","train: 1.0, loss: 0.0026477694045752287\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.002891869517043233\n","Epoch: 193\n","train: 1.0, loss: 0.0026143265422433615\n","Train Acc: 1.0 Test Acc: 0.828 Loss: 0.002854852704331279\n","Epoch: 194\n","train: 1.0, loss: 0.002581429434940219\n","Train Acc: 1.0 Test Acc: 0.827 Loss: 0.002818403299897909\n","Epoch: 195\n","train: 1.0, loss: 0.0025490259286016226\n","Train Acc: 1.0 Test Acc: 0.827 Loss: 0.002782590454444289\n","Epoch: 196\n","train: 1.0, loss: 0.0025171637535095215\n","Train Acc: 1.0 Test Acc: 0.827 Loss: 0.002747330581769347\n","Epoch: 197\n","train: 1.0, loss: 0.002485785400494933\n","Train Acc: 1.0 Test Acc: 0.827 Loss: 0.0027126215863972902\n","Epoch: 198\n","train: 1.0, loss: 0.0024549197405576706\n","Train Acc: 1.0 Test Acc: 0.827 Loss: 0.0026784681249409914\n","Epoch: 199\n","train: 1.0, loss: 0.002424495294690132\n","Train Acc: 1.0 Test Acc: 0.827 Loss: 0.002644860651344061\n"],"name":"stdout"}]}]}