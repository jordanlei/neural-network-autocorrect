{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Updated_LSTM.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"rsXBjJ8RCHFG","colab_type":"text"},"cell_type":"markdown","source":["# Imports and Setup"]},{"metadata":{"id":"F65KdYpwB2yK","colab_type":"code","outputId":"a88f0a9f-6bb9-4120-9617-96bf54811e6a","executionInfo":{"status":"ok","timestamp":1556214330525,"user_tz":240,"elapsed":12500,"user":{"displayName":"Daniel Stekol","photoUrl":"","userId":"01871686357149402115"}},"colab":{"base_uri":"https://localhost:8080/","height":243}},"cell_type":"code","source":["!git clone https://github.com/cis700/hw1-release.git\n","!mv hw1-release/dills/* .\n","!mv hw1-release hw1\n","!pip install pymagnitude\n","!pip install nltk\n","!pip install symspellpy\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import learning_curve\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import torch\n","import torchvision\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset\n","import torchvision.transforms as transforms\n","from torch.autograd import Variable\n","import torch\n","import time\n","import nltk\n","from nltk.metrics.distance import edit_distance, jaccard_distance\n","import threading\n","import random\n","from nltk.stem import PorterStemmer\n","from pymagnitude import *\n","import datetime\n","import pickle\n","from tqdm import tqdm\n","from symspellpy.symspellpy import SymSpell, Verbosity\n","from pymagnitude import *\n","import ast\n","\n","\n","\n","from hw1.helper import Logger\n","\n","\n","##MISSING NLTK DOWNLOAD\n","nltk.download(\"brown\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["fatal: destination path 'hw1-release' already exists and is not an empty directory.\n","mv: cannot stat 'hw1-release/dills/*': No such file or directory\n","mv: cannot move 'hw1-release' to 'hw1/hw1-release': Directory not empty\n","Requirement already satisfied: pymagnitude in /usr/local/lib/python3.6/dist-packages (0.1.120)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n","Requirement already satisfied: symspellpy in /usr/local/lib/python3.6/dist-packages (6.3.8)\n","Requirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.6/dist-packages (from symspellpy) (1.16.3)\n","[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Package brown is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":4}]},{"metadata":{"id":"AMwtqfc4CLzK","colab_type":"text"},"cell_type":"markdown","source":["# Tensorboard"]},{"metadata":{"id":"ugnCvNWmCPzc","colab_type":"code","colab":{}},"cell_type":"code","source":["#! rm -r ./logs\n","LOG_DIR = './logs'\n","get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","    .format(LOG_DIR)\n",")\n","\n","!if [ -f ngrok ] ; then echo \"Ngrok already installed\" ; else wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip > /dev/null 2>&1 && unzip ngrok-stable-linux-amd64.zip > /dev/null 2>&1 ; fi\n","\n","get_ipython().system_raw('./ngrok http 6006 &')\n","\n","! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print('Tensorboard Link: ' +str(json.load(sys.stdin)['tunnels'][0]['public_url']))\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"pR1j5_h0CRgs","colab_type":"text"},"cell_type":"markdown","source":["# Google Drive"]},{"metadata":{"id":"EYfssDtqCT5z","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","local_dir= 'drive/My Drive/CIS700-004/project'\n","%cd 'drive/My Drive/CIS700-004/project'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DYTZA1asCvd5","colab_type":"text"},"cell_type":"markdown","source":["# Data Generator"]},{"metadata":{"id":"GhBJc9N9CyM4","colab_type":"code","colab":{}},"cell_type":"code","source":["#initialize spell suggester\n","sym_spell = SymSpell(3, 6, compact_level=0)\n","sym_spell.load_dictionary(\"frequency_dictionary_en_82_765.txt\", 0, 1)\n","\n","#max number of candidates to consider as a replacement for a word\n","candLimit = 100\n","\n","#list of all parts of speech which are allowed to be replaced when part-of-speech filtering is on\n","switch_tags = [\"JJ\", \"JJR\", \"JJS\", \"NN\", \"NNS\", \"RB\", \"RBR\", \"RBS\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n","\n","#list of all nltk pos tags (other than punctuation)\n","all_pos = [\"CC\", \"DT\", \"EX\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"MD\", \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"PRP$\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\", \"WDT\", \"WP\", \"WP$\", \"WRB\"]\n","\n","def corrupt(tokens, tags, dist_threshold=3, pos_filter=True):\n","    \"\"\"replaces (num_words) words in the (sentence) with \n","    similar words that are only (dist_threshold) away, \n","    with optional filtering by part-of-speech\"\"\"\n","    \n","    #initialize labels (1 for correct words, 0 for out-of-place word)\n","    labels = [1]*len(tags)\n","    \n","    #gets indices of words that can be replaced (must have suitable part-of-speech), \n","    #or all indices of real words (non-punctuation/symbols) if pos_filter is false\n","    replaceables = getReplaceables(tags, pos_filter)\n","    real_word = \"\"\n","    \n","    #if there are words that can be replaced, replace one\n","    if(len(replaceables)>0):\n","        replace_ind = random.choice(replaceables)\n","        real_word = tokens[replace_ind]\n","        fake_word = suggestWord(real_word, tags[replace_ind], dist_threshold, pos_filter=pos_filter)\n","        if(fake_word!=real_word):\n","                labels[replace_ind] = 0\n","                tokens[replace_ind] = fake_word\n","    return tokens, labels, real_word\n","\n","def getReplaceables(pos_tags, filtering):\n","    \"\"\"returns indices of tokens that can be replaced (restricted set if pos_filtering is on)\"\"\"\n","    if(filtering):\n","      replaceables = [i for i,t in enumerate(pos_tags) if t in switch_tags]\n","    else:\n","      replaceables = [i for i,t in enumerate(pos_tags) if t in all_pos]\n","    return replaceables\n","\n","def suggestWord(real_word, real_tag, e_thresh=3, pos_filter=True):\n","    \"\"\"suggests a word at most edit distance e_thresh away, with optional pos_filtering\n","    (enforces that new word has same pos as original word)\"\"\"\n","    \n","    #get similarly spelled words\n","    suggestions = sym_spell.lookup(real_word, Verbosity.ALL, e_thresh)\n","    \n","    #initialize list of word candidates and frequencies\n","    candidates = [\"\"] * len(suggestions)\n","    candFreq = [0]*len(suggestions)\n","    numSuggest = 0\n","    \n","    #loop through all word candidates\n","    for s in suggestions:\n","        \n","        #add the current word to the candidate list if it has correct pos, or if pos filtering is off\n","        tag=\"\"\n","        if(pos_filter):\n","            _, tag = nltk.pos_tag([s.term])[0]\n","        if((tag==real_tag or (not pos_filter)) and s.term!=real_word):  \n","            candidates[numSuggest] = s.term\n","            candFreq[numSuggest] = s.count\n","            numSuggest+=1\n","            \n","            #stop searching if maximum number of candidates is reached\n","            if(numSuggest>candLimit):\n","                break\n","                \n","    #if there is a suitable replacement word, return it; otherwise, return original word\n","    if(numSuggest>0):\n","        return random.choices(candidates[0:numSuggest], weights=candFreq[0:numSuggest], k=1)[0]\n","    else:\n","        return real_word\n","\n","\n","def createData(output_file, num_cycles, pos_filter):\n","    \"\"\"samples sentences from brown corpus for num_cycles, and writes corrupted data to output_file.\"\"\"\n","    \n","    #loads brown corpus\n","    brown = nltk.corpus.brown\n","    allFiles = brown.fileids()\n","    \n","    #max number of sentences to pull from any given file at a time\n","    sent_per_file = 20\n","    \n","    #min sentence length to be considered valid sentence\n","    sent_threshold = 5\n","    \n","    with open(output_file, 'a') as f:\n","        for j in range(num_cycles):\n","            \n","            #randomly choose file and get pos-tagged sentences \n","            currentFile = random.choice(allFiles)            \n","            sentences = brown.tagged_sents(currentFile)\n","            sentences = [s for s in sentences if len(s)>=sent_threshold]\n","            \n","            for i in tqdm(range(sent_per_file)):\n","                #randomly choose sentence, corrupt it, \n","                #then write sentence, label, and real word to output file\n","                tagged_sent = random.choice(sentences)\n","                tokens, tags = zip(*tagged_sent)\n","                tokens, tags = list(tokens), list(tags)\n","                new_sent, labels, real_word = corrupt(tokens, tags, pos_filter=pos_filter)\n","                f.write(str(new_sent)+\"\\n\"+str(labels)+\"\\n\"+real_word+\"\\n\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4sSRSQeYq_WB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"61a2ff55-d7f8-432e-d142-383a3efdafb6","executionInfo":{"status":"ok","timestamp":1556215583618,"user_tz":240,"elapsed":458,"user":{"displayName":"Daniel Stekol","photoUrl":"","userId":"01871686357149402115"}}},"cell_type":"code","source":["sym_spell = SymSpell(3, 6, compact_level=0)\n","sym_spell.load_dictionary(\"freqdict.pkl\", 0, 1)\n","\n","suggestions = sym_spell.lookup(\"dog\", Verbosity.ALL, 3)\n","print(suggestions)\n","with open(\"freqdict.pkl\", \"r\") as f:\n","  i=0\n","  for line in f:\n","    print(f)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[]\n"],"name":"stdout"}]},{"metadata":{"id":"Thf4WGl0G7uf","colab_type":"code","colab":{}},"cell_type":"code","source":["##EDIT TO CORRECT FILE CONVENTIONS\n","createData(\"train_filtered.txt\", 2000, True)\n","createData(\"test_filtered.txt\", 2000, True)\n","createData(\"train_unfiltered.txt\", 2000, True)\n","createData(\"test_unfiltered.txt\", 2000, True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yLObOD6IHf7O","colab_type":"code","colab":{}},"cell_type":"code","source":["def getDataset(filename, savename):\n","    \"\"\"Converts corrupted sentences to vector representations (using pymagnitude), saving to output file\"\"\"\n","    \n","    #load magnitude embeddings\n","    vectors =  Magnitude(\"GoogleNews-vectors-negative300.magnitude\")\n","    \n","    sentences = []\n","    labels = []\n","    with open(filename, \"r\") as f:\n","      \n","        #iterates through file, converts each word in sentence to vector, \n","        #builds sentences and labels lists\n","        for line in tqdm(f):\n","            sentence_words = ast.literal_eval(line)\n","            label = ast.literal_eval(f.readline())\n","            real_word = f.readline()[0:-1]\n","            sentence_vector = []\n","            for word in sentence_words:\n","                sentence_vector.append(vectors.query(word))\n","            sentences.append(sentence_vector)\n","            labels.append(label)\n","                \n","    #saves sentence vector and label objects as pickle files\n","    ##EDIT TO CORRECT FILE CONVENTION\n","    pickle.dump(sentences, open(savename+\"sentvectors.pkl\", \"wb\"))\n","    pickle.dump(labels, open(savename+\"labelvectors.pkl\", \"wb\"))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VTK6DmGLJubY","colab_type":"code","colab":{}},"cell_type":"code","source":["##RUN ON EACH DATASET"],"execution_count":0,"outputs":[]},{"metadata":{"id":"j2X_qp8jCVfV","colab_type":"text"},"cell_type":"markdown","source":["# LSTM"]},{"metadata":{"id":"riyN5sdWCY4x","colab_type":"code","colab":{}},"cell_type":"code","source":["class LSTM(nn.Module): \n","    def __init__(self, input_size, hidden_size, num_layers, bidirectional= False):\n","        super(LSTM, self).__init__()\n","        self.hidden_size= hidden_size\n","        self.num_layers= num_layers\n","        self.num_directions = 2 if bidirectional else 1\n","        self.lstm= nn.LSTM(input_size, hidden_size, \n","                           num_layers, batch_first= True, bidirectional= bidirectional)\n","        self.fc= nn.Linear(hidden_size*self.num_directions, 1)\n","        \n","        #for logging\n","        self.step = 0\n","        self.epoch = 0\n","    \n","    def forward(self, x): \n","        h0 = torch.zeros(self.num_directions*self.num_layers, 1, self.hidden_size).cuda()\n","        lstm_out, _ = self.lstm(x, (h0, c0))\n","        out = lstm_out.view(-1, lstm_out.size(2))\n","        out = F.sigmoid(self.fc(F.relu(out)))\n","        return out\n","\n","    def train(self, train_data, train_labels, max_epochs, lr= .00001):\n","        \n","        #initialize objects\n","        logger = Logger(\"./\"+modelname+\"_logs\")\n","        optimizer = torch.optim.Adam(self.parameters(), lr= lr)\n","        \n","        #BCE loss since predicting binary tag for each word\n","        criterion = torch.nn.BCELoss()\n","        \n","        for epoch in range(max_epochs):\n","            \n","            #randomize order of examples\n","            order = torch.randperm(len(train_data))\n","            for i in tqdm(range(len(train_data))):\n","                \n","                #read next sentence and labels from dataset\n","                sentence = train_data[order[i]]\n","                word_labels = train_labels[order[i]]\n","\n","                #rearrange sentence and labels for feeding into network\n","                inputs = torch.tensor(sentence).view(1, len(sentence), -1).type(torch.FloatTensor).cuda()\n","                labels = torch.tensor([word_labels]).transpose(0,1).float().cuda()\n","\n","                #backprop\n","                optimizer.zero_grad()\n","                y_pred = self.forward(inputs)\n","                loss = criterion(y_pred, labels)\n","                loss.backward()\n","                optimizer.step()\n","\n","                #log loss\n","                logger.scalar_summary(\"loss\", loss.item(), self.step)\n","                logger.writer.flush()\n","                self.step +=1\n","\n","            torch.save(self, modelname +\"/epoch\" + str(self.epoch))\n","            self.epoch +=1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pcU3QyS5H_Y0","colab_type":"code","colab":{}},"cell_type":"code","source":["##TRAIN LSTMS (4)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lYm8lt3dCtLv","colab_type":"text"},"cell_type":"markdown","source":["# Evaluation"]},{"metadata":{"id":"ytsvtMIKKLDm","colab_type":"code","colab":{}},"cell_type":"code","source":["def evaluate(sentence_file, text_file, output_file, model):\n","    results = []\n","    sentence_vects = pickle.load(open(sentence_file, \"rb\"))\n","    with open(text_file, \"r\") as f:\n","        i=0\n","        with torch.no_grad():\n","          \n","            #for each example, record the sentence, vectorized sentence, true labels, and predictions\n","            for line in tqdm(f):\n","              \n","                #read in example\n","                sentence_arr = ast.literal_eval(line)\n","                label = ast.literal_eval(f.readline())\n","                real_word = f.readline()[0:-1]\n","                sentence_vect = sentence_vects[i]\n","                \n","                #get model predictions\n","                tensor_inputs = torch.tensor(sentence_vect).view(1, len(sentence_vect), -1).type(torch.FloatTensor).cuda()\n","                out = model.forward(tensor_inputs)\n","                out = out.view(-1)\n","                out=out.cpu().tolist()\n","                \n","                #discretize predictions, with threshold at .5\n","                pred = [1 if j>.5 else 0 for j in out]\n","                \n","                #get which words were predicted to be out-of-place, and which ones actually were\n","                predicted_inapprop = [sentence_arr[j] for j in range(len(sentence_arr)) if pred[j]<.5]\n","                actual_inapprop = [sentence_arr[j] for j in range(len(sentence_arr)) if label[j]==0]\n","                \n","                #append to results list\n","                results.append({\"sentence\":sentence_arr, \n","                                \"label\":label, \n","                                \"output\":out, \n","                                \"discrete\": pred,\n","                                \"inapprop\":actual_inapprop, \n","                                \"pred_inapprop\":predicted_inapprop})\n","                i+=1\n","    pickle.dump(results, open(output_file, \"wb\"))\n","    return"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PydE9aq6MEOr","colab_type":"code","colab":{}},"cell_type":"code","source":["##RUN ON ALL MODELS, 2 DATASETS"],"execution_count":0,"outputs":[]}]}